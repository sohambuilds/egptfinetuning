# EmoGPT Fine-tuning Requirements
# Optimized for Unsloth training on A6000 48GB

# Core dependencies
torch>=2.1.0
transformers>=4.36.0
datasets>=2.14.0
accelerate>=0.24.0
peft>=0.7.0
trl>=0.7.4
bitsandbytes>=0.41.0

# Unsloth for 2x faster training
unsloth @ git+https://github.com/unslothai/unsloth.git

# Optional but recommended
wandb>=0.16.0  # For experiment tracking
tiktoken>=0.5.0  # For token counting
